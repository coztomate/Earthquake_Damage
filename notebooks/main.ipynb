{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini competition for Data Science retreat\n",
    "\n",
    "This Jupyter notebook contains the code to classify the damage_grades from the [Richter's Predictor: Modeling Earthquake Damage](https://www.drivendata.org/competitions/57/nepal-earthquake/) competition. The competition is hosted on [DrivenData](https://www.drivendata.org/competitions/57/nepal-earthquake/). The goal of the competition is to predict the level of damage to buildings caused by the 2015 Gorkha earthquake in Nepal. The data is provided by [Kathmandu Living Labs](http://www.kathmandulivinglabs.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "\n",
    "First we import the data from the csv file. We use the pandas library to read the csv file and store it in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train values shape:  (260601, 39)\n",
      "Train labels shape:  (260601, 2)\n",
      "Test values shape:  (86868, 39)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import importer\n",
    "\n",
    "raw_train_values, raw_train_labels, raw_test_values = importer.import_data(directory=\"../Data\")\n",
    "\n",
    "# Print the shapes of the data\n",
    "print(\"Train values shape: \", raw_train_values.shape)\n",
    "print(\"Train labels shape: \", raw_train_labels.shape)\n",
    "print(\"Test values shape: \", raw_test_values.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Data\n",
    "\n",
    "\n",
    "BinaryEncoder transforms categorical data by first mapping each category to a unique integer. Then this integer is converted to binary digits. Each binary digit forms a separate column, resulting in log(base 2) of the total unique categories many columns. For geographic data with more than 10K unique values, one-hot encoding would create a lot of columns, significantly increasing dimensionality.\n",
    "\n",
    "Instead, BinaryEncoder creates only 14 columns (log2(10K) â‰ˆ 14), efficiently managing space without losing significant information, making it a judicious choice for dealing with high-cardinality categories.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (260601, 81)\n",
      "Test data shape:  (86868, 81)\n",
      "Train data columns:  Index(['building_id', 'geo_level_1_id_0', 'geo_level_1_id_1',\n",
      "       'geo_level_1_id_2', 'geo_level_1_id_3', 'geo_level_1_id_4',\n",
      "       'geo_level_2_id_0', 'geo_level_2_id_1', 'geo_level_2_id_2',\n",
      "       'geo_level_2_id_3', 'geo_level_2_id_4', 'geo_level_2_id_5',\n",
      "       'geo_level_2_id_6', 'geo_level_2_id_7', 'geo_level_2_id_8',\n",
      "       'geo_level_2_id_9', 'geo_level_2_id_10', 'geo_level_3_id_0',\n",
      "       'geo_level_3_id_1', 'geo_level_3_id_2', 'geo_level_3_id_3',\n",
      "       'geo_level_3_id_4', 'geo_level_3_id_5', 'geo_level_3_id_6',\n",
      "       'geo_level_3_id_7', 'geo_level_3_id_8', 'geo_level_3_id_9',\n",
      "       'geo_level_3_id_10', 'geo_level_3_id_11', 'geo_level_3_id_12',\n",
      "       'geo_level_3_id_13', 'count_floors_pre_eq', 'age', 'area_percentage',\n",
      "       'height_percentage', 'land_surface_condition_0',\n",
      "       'land_surface_condition_1', 'foundation_type_0', 'foundation_type_1',\n",
      "       'foundation_type_2', 'roof_type_0', 'roof_type_1',\n",
      "       'ground_floor_type_0', 'ground_floor_type_1', 'ground_floor_type_2',\n",
      "       'other_floor_type_0', 'other_floor_type_1', 'other_floor_type_2',\n",
      "       'position_0', 'position_1', 'position_2', 'plan_configuration_0',\n",
      "       'plan_configuration_1', 'plan_configuration_2', 'plan_configuration_3',\n",
      "       'has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone',\n",
      "       'has_superstructure_stone_flag',\n",
      "       'has_superstructure_cement_mortar_stone',\n",
      "       'has_superstructure_mud_mortar_brick',\n",
      "       'has_superstructure_cement_mortar_brick', 'has_superstructure_timber',\n",
      "       'has_superstructure_bamboo', 'has_superstructure_rc_non_engineered',\n",
      "       'has_superstructure_rc_engineered', 'has_superstructure_other',\n",
      "       'legal_ownership_status_0', 'legal_ownership_status_1',\n",
      "       'legal_ownership_status_2', 'count_families', 'has_secondary_use',\n",
      "       'has_secondary_use_agriculture', 'has_secondary_use_hotel',\n",
      "       'has_secondary_use_rental', 'has_secondary_use_institution',\n",
      "       'has_secondary_use_school', 'has_secondary_use_industry',\n",
      "       'has_secondary_use_health_post', 'has_secondary_use_gov_office',\n",
      "       'has_secondary_use_use_police', 'has_secondary_use_other'],\n",
      "      dtype='object')\n",
      "Test data columns:  Index(['building_id', 'geo_level_1_id_0', 'geo_level_1_id_1',\n",
      "       'geo_level_1_id_2', 'geo_level_1_id_3', 'geo_level_1_id_4',\n",
      "       'geo_level_2_id_0', 'geo_level_2_id_1', 'geo_level_2_id_2',\n",
      "       'geo_level_2_id_3', 'geo_level_2_id_4', 'geo_level_2_id_5',\n",
      "       'geo_level_2_id_6', 'geo_level_2_id_7', 'geo_level_2_id_8',\n",
      "       'geo_level_2_id_9', 'geo_level_2_id_10', 'geo_level_3_id_0',\n",
      "       'geo_level_3_id_1', 'geo_level_3_id_2', 'geo_level_3_id_3',\n",
      "       'geo_level_3_id_4', 'geo_level_3_id_5', 'geo_level_3_id_6',\n",
      "       'geo_level_3_id_7', 'geo_level_3_id_8', 'geo_level_3_id_9',\n",
      "       'geo_level_3_id_10', 'geo_level_3_id_11', 'geo_level_3_id_12',\n",
      "       'geo_level_3_id_13', 'count_floors_pre_eq', 'age', 'area_percentage',\n",
      "       'height_percentage', 'land_surface_condition_0',\n",
      "       'land_surface_condition_1', 'foundation_type_0', 'foundation_type_1',\n",
      "       'foundation_type_2', 'roof_type_0', 'roof_type_1',\n",
      "       'ground_floor_type_0', 'ground_floor_type_1', 'ground_floor_type_2',\n",
      "       'other_floor_type_0', 'other_floor_type_1', 'other_floor_type_2',\n",
      "       'position_0', 'position_1', 'position_2', 'plan_configuration_0',\n",
      "       'plan_configuration_1', 'plan_configuration_2', 'plan_configuration_3',\n",
      "       'has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone',\n",
      "       'has_superstructure_stone_flag',\n",
      "       'has_superstructure_cement_mortar_stone',\n",
      "       'has_superstructure_mud_mortar_brick',\n",
      "       'has_superstructure_cement_mortar_brick', 'has_superstructure_timber',\n",
      "       'has_superstructure_bamboo', 'has_superstructure_rc_non_engineered',\n",
      "       'has_superstructure_rc_engineered', 'has_superstructure_other',\n",
      "       'legal_ownership_status_0', 'legal_ownership_status_1',\n",
      "       'legal_ownership_status_2', 'count_families', 'has_secondary_use',\n",
      "       'has_secondary_use_agriculture', 'has_secondary_use_hotel',\n",
      "       'has_secondary_use_rental', 'has_secondary_use_institution',\n",
      "       'has_secondary_use_school', 'has_secondary_use_industry',\n",
      "       'has_secondary_use_health_post', 'has_secondary_use_gov_office',\n",
      "       'has_secondary_use_use_police', 'has_secondary_use_other'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import encoder\n",
    "\n",
    "fitted_enc = encoder.create_encoder(raw_train_values)\n",
    "\n",
    "train_data = encoder.encode(raw_train_values, fitted_enc)\n",
    "test_data = encoder.encode(raw_test_values, fitted_enc)\n",
    "\n",
    "\n",
    "# Print the shapes of the new data\n",
    "print(\"Train data shape: \", train_data.shape)\n",
    "print(\"Test data shape: \", test_data.shape)\n",
    "\n",
    "# Print the columns of the new data\n",
    "print(\"Train data columns: \", train_data.columns)\n",
    "print(\"Test data columns: \", test_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data\n",
    "\n",
    "We merge the data and the labels into a single dataframe. We then drop the columns that are not required for training. Like Building_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (260601, 81)\n"
     ]
    }
   ],
   "source": [
    "import cleaner\n",
    "\n",
    "train_cleaned = cleaner.clean(train_data, raw_train_labels)\n",
    "\n",
    "# Print the shapes of the new data\n",
    "print(\"Train data shape: \", train_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Data\n",
    "\n",
    "We normalize the data to bring all the features to the same scale. This helps in faster convergence of the model. And improved our accuracy considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import normalizer\n",
    "\n",
    "train_normalized, test_data = normalizer.normalize(train_cleaned, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CV Split\n",
    "\n",
    "We split the data into training and cross validation sets. We use the training set to train the model and the cross validation set to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitter\n",
    "\n",
    "X_train, X_val, y_train, y_val = splitter.split(train_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing the Model\n",
    "\n",
    "We've developed a model using the XGBoost algorithm. For training, we employed our training set, while the model's effectiveness was evaluated using a cross-validation set. This same cross-validation set was instrumental in fine-tuning the model's hyperparameters. Key hyperparameters adopted include:\n",
    "\n",
    "- Number of Estimators ('n_estimators'): 535\n",
    "- Maximum Depth of Trees ('max_depth'): 10\n",
    "- Learning Rate ('learning_rate'): 0.051232294238614126\n",
    "- Subsample Ratio ('subsample'): 0.6796645277288101\n",
    "- Ratio of Columns Sampled ('colsample_bytree'): 0.7886065868653529\n",
    "\n",
    "For our specific 3-class multi-classification problem, the objective was set to 'multi:softmax'. Thus, 'num_class' was declared as 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "\n",
    "model = model.XGBoost(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model\n",
    "\n",
    "We evaluate the model using the cross validation set. We use the accuracy score to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 score: 0.74706\n",
      "Accuracy: 0.74706\n"
     ]
    }
   ],
   "source": [
    "import evaluator\n",
    "\n",
    "accuracy, micro_f1 = evaluator.evaluate(model, X_val, y_val)\n",
    "\n",
    "print(f'Micro F1 score: {micro_f1:.5f}')\n",
    "print(f'Accuracy: {accuracy:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Without Data Splitting\n",
    "\n",
    "In an additional experiment, we trained our model without partitioning the dataset into training and cross-validation subsets. This approach surprisingly resulted in improved accuracy. However, it's worth noting that we lacked the means to appropriately measure this new model's accuracy due to the absence of a separate validation set. Regardless, owing to the observed enhancement in accuracy, we proceeded to submit our entry for the challenge. This model was the best performing model in the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "\n",
    "# Split the data\n",
    "X_train = train_normalized.drop(columns=['damage_grade'])\n",
    "y_train = train_normalized['damage_grade']\n",
    "\n",
    "# Train the model\n",
    "model = model.XGBoost(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting\n",
    "\n",
    "We predict the labels for the test data using the model we trained. The predictions are on the test_data provided by the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import predictor\n",
    "\n",
    "predictions = predictor.predict(model, test_data)\n",
    "\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit\n",
    "\n",
    "We submit the predictions to the competition. The predictions are in the form of a csv file. They can be found in the submission folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions with timestamp to folder Submissions\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "filename = f'predicted_{timestamp}.csv'\n",
    "\n",
    "predictions.to_csv(f'../Submissions/{filename}', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance\n",
    "\n",
    "We plot the features with more importance in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importance scores\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display feature names and their importance scores\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})\n",
    "\n",
    "# Sort the DataFrame by importance score in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Select the top 15 most important features\n",
    "top_15_features = feature_importance_df.head(15)\n",
    "\n",
    "# Plot the top 15 most important features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_15_features['Feature'], top_15_features['Importance'])\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 15 Most Important Features')\n",
    "plt.gca().invert_yaxis()  # Invert the y-axis to show the most important features at the top\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
